Fairness Audit Report: COMPAS Recidivism Dataset
The COMPAS dataset contains criminal justice records used to predict the likelihood of recidivism. It has been widely criticized for racial bias, particularly in risk scores assigned to African‑American defendants. Using IBM’s AI Fairness 360 toolkit, we conducted a practical audit to evaluate disparities in predictive outcomes.
We trained a logistic regression model on features such as age, sex, prior convictions, and charge degree, with the target variable being two‑year recidivism. Race was treated as the protected attribute, with Caucasian defendants considered privileged and African‑American defendants unprivileged. After splitting the dataset into training and test sets, we evaluated fairness metrics on the predictions.
The results revealed measurable disparities. The false positive rate difference was approximately ‑0.20, meaning unprivileged groups experienced substantially higher false positives compared to privileged groups. The false negative rate difference was around +0.10, indicating unprivileged groups were also more likely to be incorrectly classified as not reoffending. The disparate impact ratio was below 1, and the mean difference in favorable outcomes was negative, further confirming bias against unprivileged groups. A visualization of false positive rates highlighted the gap clearly, with African‑American defendants disproportionately misclassified.
These findings demonstrate that the COMPAS dataset and derived models perpetuate racial bias. To remediate, several strategies can be applied. Pre‑processing techniques such as reweighing can adjust training data to reduce bias. In‑processing methods like adversarial debiasing can constrain the model to produce fairer outcomes. Post‑processing approaches such as equalized odds can adjust predictions to balance error rates across groups. Beyond technical fixes, policy interventions are essential: transparency in algorithmic decision‑making, regular fairness audits, and human oversight in high‑stakes contexts like criminal justice.
In conclusion, the audit confirms racial bias in COMPAS predictions. Addressing these disparities requires both algorithmic mitigation and ethical governance to ensure AI systems do not reinforce systemic inequities.